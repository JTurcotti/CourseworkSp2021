\documentclass{article}

\usepackage{jt-shorthand}

\title{CS 278 - Homework 3}
\author{Joshua Turcotti}
\begin{document}
\maketitle
\section{Statistical Distance}
\begin{alphalist}
  \renewcommand{\D}{\mathcal{D}}
  \newcommand{\Un}{\mathcal{U}_n}
\item We choose two arbitrary strings in $\set{0, 1}^n$. To define a distribution $\D$ assign a probability of $\frac{1-\e}{2^n}$ to the first, a probability of $\frac{1+\e}{2^n}$ to the second, and a probability of $\frac{1}{2^n}$ to every other string. For any function $\set{0, 1}^n \to \set{0, 1}$, the maximal difference between its probability of taking the value 1 under $\D$ and $\Un$ is $\frac{\e}{2^{n-1}} < \e$. Thus $\D$ is $\e-$pseudorandom against all functions $\set{0, 1}^n \to \set{0, 1}$.
\item Assume for contradiction that at least $\e 2^n$ strings have probability 0 of occuring under $\D$. Let $f$ be the function that is 1 on exactly those strings. Under $\D$, the probability that $f$ takes the value 1 is 0, while under $\Un$, it is $\e$. Thus $\D$ is not $\e$-pseudorandom against $f$, so by contradiction of our assumption it must be the case that the support of $\D$ has size at least $(1 - \e)2^n$.
\item Assume $d < n$. Then at most half of the strings in $\set{0, 1}^n$ are in the range of $G$. Let $f$ take the value 1 on all strings $\set{0, 1}^n$ that are not in the range of $G$. We know that under $\Un$, $f$ has probability at least 1/2 of taking the value 1, but under $G(\mathcal{U}_d)$, it has probability 0. This contradicts the assumption that $G$ $\e$-fools $f$, so it must be the case that $d \ge n$.
\item Choose any $f: \set{0, 1}^n \to \set{0, 1}$.
  \begin{align*}
    \abs{\Pr_{x\sim\D}[f(x) = 1] - \Pr_{x\sim\Un}[f(x) = 1]} &= \abs{\sum_{y \in \set{0, 1}^n} f(y)\Pr_{x\sim\D}[x = y] - \sum_{y \in \set{0, 1}^n} f(y)\Pr_{x\sim\Un}[x = y]} \\
                                                             &\le \sum_{y \in \set{0, 1}^n}f(y)\abs{\Pr_{x\sim\D}[x = y] - \Pr_{x\sim\Un}[x = y]} \\
                                                             &\le\sum_{y \in \set{0, 1}^n}\abs{\Pr_{x\sim\D}[x = y] - \Pr_{x\sim\Un}[x = y]} \\
  \end{align*}
\end{alphalist}
\section{Yao and Impagliazzo for Formulas}
\begin{alphalist}
  \renewcommand{\F}{\mathcal{F}}
\item The only adjustment to Impagliazzo's lemma that need be made is to replace $S' = \frac{S\e^2}{100n}$ with $S' = \frac{S\e^12}{100n^6}$. With this adjustment made, we can walk through every step of the proof presented in class, beginnign by assuming that for all $\d$-dense distributions $H$ there exists a formula $F$ of size at most $S'$ such that the probability $f(x) = F(x)$ is greater than $\half + \e$. Phrasing the choice of distribution $\F$ over formulas $F$ and the $\d$-dense distributions $H$ to optimize this probability as a zero sum game, the minimax theorem allows us to conclude that there exists a distribution $\F$ such that for all $\d$-dense distributions $H$ the probability $F$ and $f$ compute the same result is greater than $\half + \e$. We can similarly bound the size of the BAD set of strings $x$ on which the probability over $\F$ is at most $\half + \e$ by $\d2^n$, and then choose $t = \frac{2n}{\e^2}$ formulas i.i.d. from $\F$ and perform a Chernoff's bound, followed by an existentially instantiating choice of a specific $t$-tuple such that for all $x$ not in the BAD set, the majority of the formulas chosen agree with $f$ with high probability. We are given that a formuas exists to compute majority of $m$ inputs with size $m^6$, so $S't + t^6$ will be the size of our total formula computing $f$ with high enough probability to contradict its hardness assumption. By our choice of $S'$ this inequality holds.
\item  Given $f: \set{0, 1}^n \to \set{0, 1}$ that cannot be computed by formulas of size $s$, we will show that $f^{\oplus k}$ cannot be compute by formulas of size $sk$.
\end{alphalist}
\section{Error Correcting Codes with Relative Distance}
\begin{alphalist}
\item For vectors in $\set{-1, 1}^n$, it is clear that their innder product is equal to $c - d$, where $c$ is the number of indices $i \le n$ at which they agree and $d$ is the number at which they differ. Thus, if $d \ge n/2$ by assumption, then $c - d \le 0$.
\item Assume $m > n$. It is impossible for all $m$ vectors to be linearly independent, so, WLOG, assume $v\ind{1}$ lies in the subspace generated by $v\ind{2}, \ldots, v\ind{m}$. We have the identity $$v\ind{1} = \inn{v\ind{1}, v\ind{2}}v\ind{2} + \ldots + \inn{v\ind{1}, v\ind{m}}v\ind{m}$$ we can consider only the first coordinates of each vector in this identity, and realize that we have expressed a positive number as a linear combination of positive numbers in which all of the coefficients are non-positive. This is impossible, so we can conclude that $m \le n$.
\item We desire an error correcting code with distance $n/2$ in $\set{0, 1}^n$. We showed in part a above that the set of codewords (mapped via $0 \mapsto 1, 1 \mapsto -1$ into $\set{-1, 1}^n$) must have pairwise non-positive inner products, so part b above tells us that there can be at most $n$ codewords that begin with a 0. We can consider the alternate mapping $\set{0, 1}^n \to \set{-1, 1}^n$ that equals the above mapping on all but the first bit, but first peforms $b \mapsto 1-b$ on the first bit. We note that the result from part a still applies to this mapping, and combining with part b we can conclude that there can be at most $n$ codewords that begin with a 1, as these will be the ones under the new mapping that have positive first coordinates after the mapping. By a union bound, there are at most $2n$ codewords.
\end{alphalist}

\end{document}
